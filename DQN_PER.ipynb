{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_PER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinHuang8/CS159-WTA/blob/master/DQN_PER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iKFM3HVilaMB",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FFdaczjmlcEK",
        "colab": {}
      },
      "source": [
        "from gym import error, spaces, utils\n",
        "from gym.utils import seeding\n",
        "\n",
        "# Create a custom environment \n",
        "class WTAEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['console']}\n",
        "\n",
        "    def __init__(self, assignment, target_values, prob, device):\n",
        "        ''' \n",
        "        Assignment - 1D array that maps weapons to assigned target\n",
        "        target_values - 1D array that maps targets to their value when destroyed\n",
        "        prob - m x n array where prob[i, j] = probability of weapon i killing target j\n",
        "        ''' \n",
        "        super(WTAEnv, self).__init__()\n",
        "        \n",
        "        self.n = len(target_values)\n",
        "        self.m = len(assignment)\n",
        "\n",
        "        self.start_assignment = assignment\n",
        "        self.assignment = assignment\n",
        "        \n",
        "        # one hot encoded version of assignment, for quick reward computation\n",
        "        self.assignment_onehot = self.get_onehot_assignments()\n",
        "        \n",
        "        self.target_values = target_values \n",
        "        self.prob = prob\n",
        "        # q = probability array of survival\n",
        "        self.q = 1 - self.prob\n",
        "        \n",
        "        # Current expected value of the assignment\n",
        "        self.value = self.assignment_value()\n",
        "        \n",
        "        # The action space - a number 0 <= i < m * n, where\n",
        "        # (weapon = i // n, target = i % n)\n",
        "        # Assigns weapon to target (one target per weapon)\n",
        "        self.action_space = spaces.MultiDiscrete([self.m * self.n])\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "    def decode_action(self, action):\n",
        "        '''\n",
        "        Given an action, return the weapon and target associated with\n",
        "        that action.\n",
        "        '''\n",
        "        return action // self.n, action % self.n\n",
        "        \n",
        "    def get_onehot_assignments(self):\n",
        "        '''\n",
        "        Get a one-hot encoded representation of the assignments.\n",
        "        This is used only because this representation is convenient and\n",
        "        fast for computing the assignment value, since we can use\n",
        "        vectorization and not use for loops. Not sure how much this\n",
        "        actually helps though.\n",
        "        '''\n",
        "        onehot = np.zeros((self.m, self.n))\n",
        "        onehot[np.arange(onehot.shape[0]), self.assignment] = 1\n",
        "        return onehot\n",
        " \n",
        "    def assignment_value(self):\n",
        "        '''\n",
        "        Compute the expected value of our assignment.\n",
        "        E = Sum over targets i [P(target i killed) * Value(i)]\n",
        "        where P(target i killed) = 1 - P(i survives)\n",
        "        where P(i surves) = 1 - Product over weapons j [P(i survives j) = q[i, j]]\n",
        "        '''\n",
        "        pkill = 1 - np.prod(self.q ** self.assignment_onehot, axis=0)\n",
        "        expected_value = np.dot(pkill, self.target_values)\n",
        "        return expected_value\n",
        "        \n",
        "    def step(self, action):\n",
        "        '''Perform action on the current state'''\n",
        "        weapon, target = self.decode_action(action[0])\n",
        "        if weapon < 0 or weapon >= self.m or target < 0 or target >= self.n:\n",
        "            raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
        "        # Update assignments\n",
        "        old_target = self.assignment[weapon]\n",
        "        self.assignment[weapon] = target\n",
        "        self.assignment_onehot[weapon, old_target] = 0\n",
        "        self.assignment_onehot[weapon, target] = 1\n",
        "\n",
        "        # Reward is change in assignment value\n",
        "        new_value = self.assignment_value()\n",
        "        reward = new_value - self.value\n",
        "        self.value = new_value\n",
        "\n",
        "        # There is no stopping condition other than the max number of iterations\n",
        "        done = False\n",
        "\n",
        "        return (self.get_state(), reward, done, {})\n",
        "\n",
        "    def get_state(self):\n",
        "        '''\n",
        "        Our state is a size m + n array.\n",
        "        state[:m] is self.assignments\n",
        "        state[m:] is self.target_values\n",
        "        '''\n",
        "        state = np.concatenate([self.assignment, self.target_values])\n",
        "        return torch.tensor(state, device=self.device)\n",
        "\n",
        "    def reset(self):\n",
        "        '''\n",
        "        Important: the observation must be a numpy array\n",
        "        :return: (np.array) \n",
        "        '''\n",
        "        self.assignment = self.start_assignment\n",
        "        self.assignment_onehot = self.get_onehot_assignments()\n",
        "        return self.get_state()\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvExsnAaTCUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SumTree(object):\n",
        "    write = 0\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.tree = np.zeros( 2*capacity - 1 )\n",
        "        self.data = np.zeros( capacity, dtype=object )\n",
        "\n",
        "    def _propagate(self, idx, change):\n",
        "        parent = (idx - 1) // 2\n",
        "\n",
        "        self.tree[parent] += change\n",
        "\n",
        "        if parent != 0:\n",
        "            self._propagate(parent, change)\n",
        "\n",
        "    def _retrieve(self, idx, s):\n",
        "        left = 2 * idx + 1\n",
        "        right = left + 1\n",
        "\n",
        "        if left >= len(self.tree):\n",
        "            return idx\n",
        "\n",
        "        if s <= self.tree[left]:\n",
        "            return self._retrieve(left, s)\n",
        "        else:\n",
        "            return self._retrieve(right, s-self.tree[left])\n",
        "\n",
        "    def total(self):\n",
        "        return self.tree[0]\n",
        "\n",
        "    def add(self, p, data):\n",
        "        idx = self.write + self.capacity - 1\n",
        "\n",
        "        self.data[self.write] = data\n",
        "        self.update(idx, p)\n",
        "\n",
        "        self.write += 1\n",
        "        if self.write >= self.capacity:\n",
        "            self.write = 0\n",
        "\n",
        "    def update(self, idx, p):\n",
        "        change = p - self.tree[idx]\n",
        "\n",
        "        self.tree[idx] = p\n",
        "        self._propagate(idx, change)\n",
        "\n",
        "    def get(self, s):\n",
        "        idx = self._retrieve(0, s)\n",
        "        dataIdx = idx - self.capacity + 1\n",
        "\n",
        "        return (idx, self.tree[idx], self.data[dataIdx])\n",
        "    \n",
        "    def len(self):\n",
        "        length = (self.tree!=0).argmin()\n",
        "        if (length == 0 and self.tree[0] != 0):\n",
        "          return self.capacity\n",
        "        return length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlUFzmsLqC4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Take in n by m matrix, convert it to 1D feature vector '''\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, n, m, embedding_size=8):\n",
        "        super(DQN, self).__init__()\n",
        "        # The assignment becomes embedded, so it has size m * embedding_size\n",
        "        # when flattened\n",
        "        # The n comes from the values attached\n",
        "        self.assignment_size = m * embedding_size\n",
        "        self.input_size = self.assignment_size + n\n",
        "        self.output_size = m * n\n",
        "        self.n = n\n",
        "        self.m = m\n",
        "        \n",
        "        units = 128\n",
        "\n",
        "        self.embedding_size = embedding_size\n",
        "        # Embed the targets, since the actual numerical value of the\n",
        "        # targets don't mean anything\n",
        "        # Another idea: skip the middleman and replace the targets\n",
        "        # with the target values\n",
        "        self.embedding = nn.Embedding(n, self.embedding_size)\n",
        "        self.lin1 = nn.Linear(self.input_size, units)\n",
        "        self.drop1 = nn.Dropout(0.3)\n",
        "        self.lin2 = nn.Linear(units, self.output_size)\n",
        "        self.drop2 = nn.Dropout(0.2)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, state):\n",
        "        assignment = state[:, :self.m].long()\n",
        "        assignment = self.embedding(assignment)\n",
        "        \n",
        "        values = state[:, self.m:].float()\n",
        "                \n",
        "        # Flatten the assignment embedding\n",
        "        assignment = assignment.view(-1, self.assignment_size).float() \n",
        "        \n",
        "        # and concatenate the values\n",
        "        x = torch.cat([assignment, values], dim=1)\n",
        "        \n",
        "        x = F.relu(self.drop1(self.lin1(x)))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp1mci6CqB-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with dueling networks\n",
        "class DuelingDQN(nn.Module):\n",
        "\n",
        "    def __init__(self, n, m, embedding_size=8, units=128):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        # The assignment becomes embedded, so it has size m * embedding_size\n",
        "        # when flattened\n",
        "        # The n comes from the values attached\n",
        "        self.assignment_size = m * embedding_size\n",
        "        self.input_size = self.assignment_size + n\n",
        "        self.output_size = m * n\n",
        "        self.n = n\n",
        "        self.m = m\n",
        "      \n",
        "        self.units = units\n",
        "\n",
        "        self.embedding_size = embedding_size\n",
        "        # Embed the targets, since the actual numerical value of the\n",
        "        # targets don't mean anything\n",
        "        # Another idea: skip the middleman and replace the targets\n",
        "        # with the target values\n",
        "        self.embedding = nn.Embedding(n, self.embedding_size)\n",
        "\n",
        "        # Layer to measure the value of a state\n",
        "        self.value_stream = nn.Sequential(\n",
        "            nn.Linear(self.input_size, units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(units, 1)\n",
        "        )\n",
        "        # Layer to measure the advantages of an action given a state\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            nn.Linear(self.input_size, units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(units, self.output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        assignment = state[:, :self.m].long()\n",
        "        assignment = self.embedding(assignment)\n",
        "\n",
        "\n",
        "        values = state[:, self.m:].float()\n",
        "\n",
        "        # Flatten the assignment embedding\n",
        "        assignment = assignment.view(-1, self.assignment_size).float() \n",
        "        \n",
        "        # and concatenate the values\n",
        "        x = torch.cat([assignment, values], dim=1)\n",
        "        values = self.value_stream(x)\n",
        "        advantages = self.advantage_stream(x)\n",
        "        qvals = values + (advantages - advantages.mean())\n",
        "        \n",
        "        return qvals\n",
        "\n",
        "    def feature_size(self):\n",
        "        return self.conv(autograd.Variable(torch.zeros(1, *self.input_dim))).view(1, -1).size(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V3QTy1Eqlndm",
        "colab": {}
      },
      "source": [
        "def generate_initial_assignment(n, m):\n",
        "    '''   \n",
        "    Randomly assign weapons to targets\n",
        "    ''' \n",
        "    assignment = np.random.randint(n, size=m)\n",
        "    return assignment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "brm37nR4lptv",
        "colab": {}
      },
      "source": [
        "episode_durations = []\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if is_ipython:\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAwMH3jwSd_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "steps_done = 0\n",
        "\n",
        "def is_possible(state, weapon, target):\n",
        "    '''\n",
        "    We don't want to assign a weapon to the target if it is already assigned\n",
        "    to the target, since this does not change the state at all.\n",
        "    '''\n",
        "    curr_target = state[weapon].item()\n",
        "    return curr_target != target \n",
        "\n",
        "'''\n",
        "Takes a state and returns an action and boolean which is True iff the model exploited\n",
        "'''\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        ## Exploit\n",
        "        with torch.no_grad():\n",
        "            policy_net.eval()\n",
        "            state_batch = torch.unsqueeze(state, 1).transpose(0, 1).float()\n",
        "            largest = torch.sort(policy_net(state_batch), descending=True, dim=1)[1]\n",
        "            policy_net.train()\n",
        "            \n",
        "            # Try until we get a valid action\n",
        "            for i in largest[0]:\n",
        "                weapon = i / n\n",
        "                target = i % n\n",
        "                \n",
        "                if is_possible(state, weapon.item(), target.item()):\n",
        "                    return torch.tensor([i], device=device), True\n",
        "                \n",
        "            # This should never happen\n",
        "            raise ValueError('Invalid state: no possible action')\n",
        "    else:\n",
        "        ## Explore\n",
        "        weapon = np.random.randint(m)\n",
        "        curr_target = state[weapon].item()\n",
        "        target = np.random.randint(n)\n",
        "        while target == curr_target:\n",
        "            target = np.random.randint(n)\n",
        "        \n",
        "        action = weapon * n + target\n",
        "        return torch.tensor([action], device=device, dtype=torch.long), False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2CdnpPbaU7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Takes a list of transitions and returns a list of new priority values\n",
        "'''\n",
        "def get_error(transitions, policy_net, target_net):\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch, state_batch, action_batch, reward_batch = None, None, None, None\n",
        "    if len(transitions) == 1:\n",
        "      batch = transitions[0]\n",
        "    else:\n",
        "      batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.stack([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "        \n",
        "    \n",
        "    if len(transitions) == 1:\n",
        "      state_batch = batch.state.unsqueeze(0)\n",
        "      action_batch = batch.action.unsqueeze(0)\n",
        "      reward_batch = batch.reward.unsqueeze(0)\n",
        "      if batch.next_state is not None:\n",
        "        non_final_mask = torch.tensor(True, device=device, dtype=torch.bool)\n",
        "        non_final_next_states = batch.next_state.unsqueeze(0)\n",
        "      else:\n",
        "        non_final_mask = torch.tensor(False, device=device, dtype=torch.bool)\n",
        "        non_final_next_states = torch.stack([])\n",
        "    else:\n",
        "      state_batch = torch.stack(batch.state)\n",
        "      action_batch = torch.stack(batch.action)\n",
        "      reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(len(transitions), device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "#     print(next_state_values[non_final_mask].shape)\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    \n",
        "\n",
        "    # Compute Huber loss\n",
        "    loss = F.smooth_l1_loss(state_action_values.double(), expected_state_action_values.unsqueeze(1).double())\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dXAtvLthltku",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if memory.num_stored() < BATCH_SIZE:\n",
        "        return\n",
        "    transitions, ids = memory.sample(BATCH_SIZE) # NEED TO GET IS_WEIGHTS\n",
        "    error = get_error(transitions, policy_net, target_net)\n",
        "    for i in range(len(ids)):\n",
        "      memory.update(ids[i], error[i])\n",
        "    \n",
        "    value_loss = error.item()\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "    \n",
        "    return value_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxw-zfrzzTAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 50\n",
        "TARGET_UPDATE = 200\n",
        "MAX_ITERATIONS = 500\n",
        "\n",
        "# n - number of targets\n",
        "n = 4\n",
        "# m - number of weapons\n",
        "m = 5\n",
        "assert n > 1\n",
        "\n",
        "lower_val = 25\n",
        "upper_val = 50\n",
        "lower_prob = 0.6\n",
        "upper_prob = 0.9\n",
        "values = np.random.uniform(lower_val, upper_val, n)\n",
        "prob = np.random.uniform(lower_prob, upper_prob, (m, n))\n",
        "assignment = generate_initial_assignment(n, m)\n",
        "env = WTAEnv(assignment, values, prob, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lseZmDBjOXe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#policy_net = DQN(n, m, n // 2).to(device)\n",
        "#target_net = DQN(n, m, n // 2).to(device)\n",
        "policy_net = DuelingDQN(n, m).to(device)\n",
        "target_net = DuelingDQN(n, m).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters())\n",
        "memory = ReplayMemory(10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDROzGYAynQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.tree = SumTree(capacity)\n",
        "        self.alpha = 0.6\n",
        "        self.min_priority = 0.01\n",
        "\n",
        "    def push(self, priority, transition):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        self.tree.add(np.power(priority + self.min_priority, self.alpha), transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        sums = np.random.randint(0, self.tree.total(), batch_size)\n",
        "        transitions = []\n",
        "        ids = []\n",
        "        for s in sums:\n",
        "          data = self.tree.get(s)\n",
        "          transitions.append(data[2])\n",
        "          ids.append(data[0])\n",
        "        return transitions, ids\n",
        "\n",
        "    def update(self, idx, priority):\n",
        "        self.tree.update(idx, priority)\n",
        "\n",
        "    def num_stored(self):\n",
        "      return self.tree.len()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bJQI_SFNl2h6",
        "outputId": "b1c9776b-13ac-4d42-d727-837f4562d947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "num_episodes = 15\n",
        "env.reset()\n",
        "init_state = env.get_state() \n",
        "for i_episode in range(num_episodes):\n",
        "    # Initialize the environment and state\n",
        "    env.reset()\n",
        "    state = init_state\n",
        "    for t in range(1, MAX_ITERATIONS+1):\n",
        "        print(f'episode {i_episode}/{num_episodes}, iteration {t}/{MAX_ITERATIONS}', end=' ')\n",
        "        # Select and perform an action\n",
        "        action, exploit = select_action(state)\n",
        "        observation, reward, done, _ = env.step(action)\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        if not done:\n",
        "            next_state = observation\n",
        "        else:\n",
        "            next_state = None\n",
        "        if exploit:\n",
        "          priority = get_error([Transition(state, action, next_state, reward)], policy_net, target_net).item()\n",
        "        else:\n",
        "          priority = abs(reward)\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(priority, Transition(state, action, next_state, reward))\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the target network)\n",
        "        loss = optimize_model()\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "        \n",
        "        print(f'loss: {loss}', end='\\r')\n",
        "    print()\n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "print()\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode 0/15, iteration 1/500 loss: None\repisode 0/15, iteration 2/500 loss: None\repisode 0/15, iteration 3/500 loss: None\repisode 0/15, iteration 4/500 loss: None\repisode 0/15, iteration 5/500 loss: None\repisode 0/15, iteration 6/500 loss: None\repisode 0/15, iteration 7/500 loss: None\repisode 0/15, iteration 8/500 loss: None\repisode 0/15, iteration 9/500 loss: None\repisode 0/15, iteration 10/500 loss: None\repisode 0/15, iteration 11/500 loss: None\repisode 0/15, iteration 12/500 loss: None\repisode 0/15, iteration 13/500 loss: None\repisode 0/15, iteration 14/500 loss: None\repisode 0/15, iteration 15/500 loss: None\repisode 0/15, iteration 16/500 loss: None\repisode 0/15, iteration 17/500 loss: None\repisode 0/15, iteration 18/500 loss: None\repisode 0/15, iteration 19/500 loss: None\repisode 0/15, iteration 20/500 loss: None\repisode 0/15, iteration 21/500 loss: None\repisode 0/15, iteration 22/500 loss: None\repisode 0/15, iteration 23/500 loss: None\repisode 0/15, iteration 24/500 loss: None\repisode 0/15, iteration 25/500 loss: None\repisode 0/15, iteration 26/500 loss: None\repisode 0/15, iteration 27/500 loss: None\repisode 0/15, iteration 28/500 loss: None\repisode 0/15, iteration 29/500 loss: None\repisode 0/15, iteration 30/500 loss: None\repisode 0/15, iteration 31/500 loss: None\repisode 0/15, iteration 32/500 loss: None\repisode 0/15, iteration 33/500 loss: None\repisode 0/15, iteration 34/500 loss: None\repisode 0/15, iteration 35/500 loss: None\repisode 0/15, iteration 36/500 loss: None\repisode 0/15, iteration 37/500 loss: None\repisode 0/15, iteration 38/500 loss: None\repisode 0/15, iteration 39/500 loss: None\repisode 0/15, iteration 40/500 loss: None\repisode 0/15, iteration 41/500 loss: None\repisode 0/15, iteration 42/500 loss: None\repisode 0/15, iteration 43/500 loss: None\repisode 0/15, iteration 44/500 loss: None\repisode 0/15, iteration 45/500 loss: None\repisode 0/15, iteration 46/500 loss: None\repisode 0/15, iteration 47/500 loss: None\repisode 0/15, iteration 48/500 loss: None\repisode 0/15, iteration 49/500 loss: None\repisode 0/15, iteration 50/500 loss: None\repisode 0/15, iteration 51/500 loss: None\repisode 0/15, iteration 52/500 loss: None\repisode 0/15, iteration 53/500 loss: None\repisode 0/15, iteration 54/500 loss: None\repisode 0/15, iteration 55/500 loss: None\repisode 0/15, iteration 56/500 loss: None\repisode 0/15, iteration 57/500 loss: None\repisode 0/15, iteration 58/500 loss: None\repisode 0/15, iteration 59/500 loss: None\repisode 0/15, iteration 60/500 loss: None\repisode 0/15, iteration 61/500 loss: None\repisode 0/15, iteration 62/500 loss: None\repisode 0/15, iteration 63/500 loss: None\repisode 0/15, iteration 64/500 loss: None\repisode 0/15, iteration 65/500 loss: None\repisode 0/15, iteration 66/500 loss: None\repisode 0/15, iteration 67/500 loss: None\repisode 0/15, iteration 68/500 loss: None\repisode 0/15, iteration 69/500 loss: None\repisode 0/15, iteration 70/500 loss: None\repisode 0/15, iteration 71/500 loss: None\repisode 0/15, iteration 72/500 loss: None\repisode 0/15, iteration 73/500 loss: None\repisode 0/15, iteration 74/500 loss: None\repisode 0/15, iteration 75/500 loss: None\repisode 0/15, iteration 76/500 loss: None\repisode 0/15, iteration 77/500 loss: None\repisode 0/15, iteration 78/500 loss: None\repisode 0/15, iteration 79/500 loss: None\repisode 0/15, iteration 80/500 loss: None\repisode 0/15, iteration 81/500 loss: None\repisode 0/15, iteration 82/500 loss: None\repisode 0/15, iteration 83/500 loss: None\repisode 0/15, iteration 84/500 loss: None\repisode 0/15, iteration 85/500 loss: None\repisode 0/15, iteration 86/500 loss: None\repisode 0/15, iteration 87/500 loss: None\repisode 0/15, iteration 88/500 loss: None\repisode 0/15, iteration 89/500 loss: None\repisode 0/15, iteration 90/500 loss: None\repisode 0/15, iteration 91/500 loss: None\repisode 0/15, iteration 92/500 loss: None\repisode 0/15, iteration 93/500 loss: None\repisode 0/15, iteration 94/500 loss: None\repisode 0/15, iteration 95/500 loss: None\repisode 0/15, iteration 96/500 loss: None\repisode 0/15, iteration 97/500 loss: None\repisode 0/15, iteration 98/500 loss: None\repisode 0/15, iteration 99/500 loss: None\repisode 0/15, iteration 100/500 loss: None\repisode 0/15, iteration 101/500 loss: None\repisode 0/15, iteration 102/500 loss: None\repisode 0/15, iteration 103/500 loss: None\repisode 0/15, iteration 104/500 loss: None\repisode 0/15, iteration 105/500 loss: None\repisode 0/15, iteration 106/500 loss: None\repisode 0/15, iteration 107/500 loss: None\repisode 0/15, iteration 108/500 loss: None\repisode 0/15, iteration 109/500 loss: None\repisode 0/15, iteration 110/500 loss: None\repisode 0/15, iteration 111/500 loss: None\repisode 0/15, iteration 112/500 loss: None\repisode 0/15, iteration 113/500 loss: None\repisode 0/15, iteration 114/500 loss: None\repisode 0/15, iteration 115/500 loss: None\repisode 0/15, iteration 116/500 loss: None\repisode 0/15, iteration 117/500 loss: None\repisode 0/15, iteration 118/500 loss: None\repisode 0/15, iteration 119/500 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:56: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode 0/15, iteration 500/500 loss: None\n",
            "episode 1/15, iteration 500/500 loss: None\n",
            "episode 2/15, iteration 500/500 loss: None\n",
            "episode 3/15, iteration 500/500 loss: None\n",
            "episode 4/15, iteration 500/500 loss: None\n",
            "episode 5/15, iteration 500/500 loss: None\n",
            "episode 6/15, iteration 500/500 loss: None\n",
            "episode 7/15, iteration 500/500 loss: None\n",
            "episode 8/15, iteration 500/500 loss: None\n",
            "episode 9/15, iteration 500/500 loss: None\n",
            "episode 10/15, iteration 500/500 loss: None\n",
            "episode 11/15, iteration 500/500 loss: None\n",
            "episode 12/15, iteration 500/500 loss: None\n",
            "episode 13/15, iteration 500/500 loss: None\n",
            "episode 14/15, iteration 500/500 loss: None\n",
            "\n",
            "Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM_itGUVzTAg",
        "colab_type": "code",
        "outputId": "f233b451-384c-471a-f104-9dec1cebfd2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "policy_net.eval()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DuelingDQN(\n",
              "  (embedding): Embedding(4, 8)\n",
              "  (value_stream): Sequential(\n",
              "    (0): Linear(in_features=44, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              "  (advantage_stream): Sequential(\n",
              "    (0): Linear(in_features=44, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=20, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC4I5LhOzTAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assignment = generate_initial_assignment(n, m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48iRlcQZzTAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_env = WTAEnv(assignment, values, prob, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPQ7sd7XzTAp",
        "colab_type": "code",
        "outputId": "a896566b-8750-44c7-817c-4f633c3d7320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_env.reset()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3.0000,  2.0000,  2.0000,  0.0000,  0.0000, 26.1013, 48.5409, 49.2651,\n",
              "        41.2852], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmMYTdaNzTAr",
        "colab_type": "code",
        "outputId": "884b2838-3052-4460-d718-fb1a7f998e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_env.value"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96.95784918889228"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtSaOyRTzTAt",
        "colab_type": "code",
        "outputId": "2732f35b-1658-4910-852b-07fb03e4735e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "state = test_env.get_state().unsqueeze(1).transpose(0, 1)\n",
        "best_action = policy_net(state).max(1)[1]\n",
        "\n",
        "a = best_action.item()\n",
        "print(a // n, a % n)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7NBo7-pzTAv",
        "colab_type": "code",
        "outputId": "a55bb702-a35b-44ef-affc-cec2d2174f03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_env.step(best_action)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 3.0000,  1.0000,  2.0000,  0.0000,  0.0000, 26.1013, 48.5409, 49.2651,\n",
              "         41.2852], dtype=torch.float64), 22.078227073341864, False, {})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY1b34mIzTAx",
        "colab_type": "code",
        "outputId": "c67cdc7c-89b5-44cb-bf44-ff12ea714bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_env.value"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "119.03607626223415"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGZXVajtzTAz",
        "colab_type": "code",
        "outputId": "3f433256-d331-41a6-d983-d09ca79d004e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "policy_net(state).reshape(5, 4)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1329, -0.9189,  1.1053,  0.2636],\n",
              "        [-1.6459,  5.7699, -1.7948,  3.9028],\n",
              "        [-2.4057, -3.2152, -1.7787, -5.6632],\n",
              "        [ 1.1704,  2.7582,  0.6964,  2.3544],\n",
              "        [ 1.5733,  0.3264,  2.2525,  0.1131]], grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0hqGZy6zTA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}