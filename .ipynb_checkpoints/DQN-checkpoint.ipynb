{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iKFM3HVilaMB"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FFdaczjmlcEK"
   },
   "outputs": [],
   "source": [
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "# Create a custom environment \n",
    "class WTAEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['console']}\n",
    "\n",
    "    def __init__(self, n, m, lower_val, upper_val, prob, device):\n",
    "        ''' \n",
    "        n - number of targets\n",
    "        m - number of weapons\n",
    "        lower_val/upper_val - lower/upper range to randomly generate target values from\n",
    "        prob - m x n array where prob[i, j] = probability of weapon i killing target j\n",
    "        ''' \n",
    "        super(WTAEnv, self).__init__()\n",
    "        \n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.device = device\n",
    "        \n",
    "        self.target_values = np.random.uniform(lower_val, upper_val, self.n) \n",
    "        self.prob = prob\n",
    "        # q = probability array of survival\n",
    "        self.q = 1 - self.prob\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "        # Current expected value of the assignment\n",
    "        self.value = self.assignment_value()\n",
    "        \n",
    "        # The action space - a number 0 <= i < m * n, where\n",
    "        # (weapon = i // n, target = i % n)\n",
    "        # Assigns weapon to target (one target per weapon)\n",
    "        self.action_space = spaces.MultiDiscrete([self.m * self.n])\n",
    "        \n",
    "    def decode_action(self, action):\n",
    "        '''\n",
    "        Given an action, return the weapon and target associated with\n",
    "        that action.\n",
    "        '''\n",
    "        return action // self.n, action % self.n\n",
    "        \n",
    "    def get_onehot_assignments(self):\n",
    "        '''\n",
    "        Get a one-hot encoded representation of the assignments.\n",
    "        This is used only because this representation is convenient and\n",
    "        fast for computing the assignment value, since we can use\n",
    "        vectorization and not use for loops. Not sure how much this\n",
    "        actually helps though.\n",
    "        '''\n",
    "        onehot = np.zeros((self.m, self.n))\n",
    "        onehot[np.arange(onehot.shape[0]), self.assignment] = 1\n",
    "        return onehot\n",
    " \n",
    "    def assignment_value(self):\n",
    "        '''\n",
    "        Compute the expected value of our assignment.\n",
    "        E = Sum over targets i [P(target i killed) * Value(i)]\n",
    "        where P(target i killed) = 1 - P(i survives)\n",
    "        where P(i surves) = 1 - Product over weapons j [P(i survives j) = q[i, j]]\n",
    "        '''\n",
    "        pkill = 1 - np.prod(self.q ** self.assignment_onehot, axis=0)\n",
    "        expected_value = np.dot(pkill, self.target_values)\n",
    "        return expected_value\n",
    "        \n",
    "    def step(self, action):\n",
    "        '''Perform action on the current state'''\n",
    "        weapon, target = self.decode_action(action[0])\n",
    "        if weapon < 0 or weapon >= self.m or target < 0 or target >= self.n:\n",
    "            raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "        # Update assignments\n",
    "        old_target = self.assignment[weapon]\n",
    "        self.assignment[weapon] = target\n",
    "        self.assignment_onehot[weapon, old_target] = 0\n",
    "        self.assignment_onehot[weapon, target] = 1\n",
    "\n",
    "        # Reward is change in assignment value\n",
    "        new_value = self.assignment_value()\n",
    "        reward = new_value - self.value\n",
    "        self.value = new_value\n",
    "\n",
    "        # There is no stopping condition other than the max number of iterations\n",
    "        done = False\n",
    "\n",
    "        return (self.get_state(), reward, done, {})\n",
    "\n",
    "    def get_state(self):\n",
    "        '''\n",
    "        Our state is a size m + n array.\n",
    "        state[:m] is self.assignments\n",
    "        state[m:] is self.target_values\n",
    "        '''\n",
    "        state = np.concatenate([self.assignment, self.target_values])\n",
    "        return torch.tensor(state, device=self.device)\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array) \n",
    "        '''\n",
    "        self.assignment = self.generate_initial_assignment()\n",
    "        self.assignment_onehot = self.get_onehot_assignments()\n",
    "        return self.get_state()\n",
    "    \n",
    "    def generate_initial_assignment(self):\n",
    "        '''   \n",
    "        Randomly assign weapons to targets\n",
    "        ''' \n",
    "        assignment = np.random.randint(self.n, size=self.m)\n",
    "        return assignment\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wG4OUdZ2lfat"
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brm37nR4lptv"
   },
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def is_possible(state, weapon, target):\n",
    "    '''\n",
    "    We don't want to assign a weapon to the target if it is already assigned\n",
    "    to the target, since this does not change the state at all.\n",
    "    '''\n",
    "    curr_target = state[weapon].item()\n",
    "    return curr_target != target \n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        ## Exploit\n",
    "        with torch.no_grad():\n",
    "            policy_net.eval()\n",
    "            state_batch = torch.unsqueeze(state, 1).transpose(0, 1).float()\n",
    "            largest = torch.sort(policy_net(state_batch), descending=True, dim=1)[1]\n",
    "            policy_net.train()\n",
    "            \n",
    "            # Try until we get a valid action\n",
    "            for i in largest[0]:\n",
    "                weapon = i / n\n",
    "                target = i % n\n",
    "                \n",
    "                if is_possible(state, weapon.item(), target.item()):\n",
    "                    return torch.tensor([i], device=device)\n",
    "                \n",
    "            # This should never happen\n",
    "            raise ValueError('Invalid state: no possible action')\n",
    "    else:\n",
    "        ## Explore\n",
    "        weapon = np.random.randint(m)\n",
    "        curr_target = state[weapon].item()\n",
    "        target = np.random.randint(n)\n",
    "        while target == curr_target:\n",
    "            target = np.random.randint(n)\n",
    "        \n",
    "        action = weapon * n + target\n",
    "        return torch.tensor([action], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dXAtvLthltku"
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "        \n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.stack([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "        \n",
    "    state_batch = torch.stack(batch.state)\n",
    "    action_batch = torch.stack(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values.double(), expected_state_action_values.unsqueeze(1).double())\n",
    "    value_loss = loss.item()\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RujMbIuTlmZK"
   },
   "outputs": [],
   "source": [
    "'''Take in n by m matrix, convert it to 1D feature vector '''\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n, m, embedding_size=8, units=64):\n",
    "        super(DQN, self).__init__()\n",
    "        # The assignment becomes embedded, so it has size m * embedding_size\n",
    "        # when flattened\n",
    "        # The n comes from the values attached\n",
    "        self.assignment_size = m * embedding_size\n",
    "        self.input_size = self.assignment_size + n\n",
    "        self.output_size = m * n\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "    \n",
    "        self.embedding_size = embedding_size\n",
    "        # Embed the targets, since the actual numerical value of the\n",
    "        # targets don't mean anything\n",
    "        # Another idea: skip the middleman and replace the targets\n",
    "        # with the target values\n",
    "        self.embedding = nn.Embedding(n, self.embedding_size)\n",
    "        self.lin1 = nn.Linear(self.input_size, units)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.lin2 = nn.Linear(units, self.output_size)\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, state):\n",
    "        assignment = state[:, :self.m].long()\n",
    "        assignment = self.embedding(assignment)\n",
    "        \n",
    "        values = state[:, self.m:].float()\n",
    "                \n",
    "        # Flatten the assignment embedding\n",
    "        assignment = assignment.view(-1, self.assignment_size).float() \n",
    "        \n",
    "        # and concatenate the values\n",
    "        x = torch.cat([assignment, values], dim=1)\n",
    "        \n",
    "        x = F.relu(self.drop1(self.lin1(x)))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with dueling networks\n",
    "class DuelingDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n, m, embedding_size=8, units=128):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        # The assignment becomes embedded, so it has size m * embedding_size\n",
    "        # when flattened\n",
    "        # The n comes from the values attached\n",
    "        self.assignment_size = m * embedding_size\n",
    "        self.input_size = self.assignment_size + n\n",
    "        self.output_size = m * n\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "      \n",
    "        self.units = units\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        # Embed the targets, since the actual numerical value of the\n",
    "        # targets don't mean anything\n",
    "        # Another idea: skip the middleman and replace the targets\n",
    "        # with the target values\n",
    "        self.embedding = nn.Embedding(n, self.embedding_size)\n",
    "        self.lin1 = nn.Linear(self.input_size, units)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "\n",
    "        # Layer to measure the value of a state\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(units, units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units, 1)\n",
    "        )\n",
    "        # Layer to measure the advantages of an action given a state\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(units, units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units, self.output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        assignment = state[:, :self.m].long()\n",
    "        assignment = self.embedding(assignment)\n",
    "\n",
    "\n",
    "        values = state[:, self.m:].float()\n",
    "\n",
    "        # Flatten the assignment embedding\n",
    "        assignment = assignment.view(-1, self.assignment_size).float() \n",
    "        \n",
    "        # and concatenate the values\n",
    "        x = torch.cat([assignment, values], dim=1)\n",
    "        x = F.relu(self.drop1(self.lin1(x)))\n",
    "        values = self.value_stream(x)\n",
    "        advantages = self.advantage_stream(x)\n",
    "        qvals = values + (advantages - advantages.mean())\n",
    "        \n",
    "        return qvals\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.conv(autograd.Variable(torch.zeros(1, *self.input_dim))).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 100\n",
    "MAX_ITERATIONS = 500\n",
    "\n",
    "# n - number of targets\n",
    "n = 10\n",
    "# m - number of weapons\n",
    "m = 10\n",
    "assert n > 1\n",
    "\n",
    "lower_val = 25\n",
    "upper_val = 50\n",
    "lower_prob = 0.6\n",
    "upper_prob = 0.9\n",
    "prob = np.random.uniform(lower_prob, upper_prob, (m, n))\n",
    "env = WTAEnv(n, m, lower_val, upper_val, prob, device)\n",
    "\n",
    "policy_net = DuelingDQN(n, m, n // 2, 64).to(device)\n",
    "target_net = DuelingDQN(n, m, n // 2, 64).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "bJQI_SFNl2h6",
    "outputId": "bd33e174-0765-4f58-87e8-472a5caaf45d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0/100, iteration 499/500 loss: 2.4298387005291127\n",
      "episode 1/100, iteration 260/500 loss: 7.7605154612867995\r"
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "# env.reset()\n",
    "# init_state = env.get_state()\n",
    "losses = []\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    state = env.get_state()\n",
    "    for t in range(MAX_ITERATIONS):\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        if not done:\n",
    "            next_state = observation\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        loss = optimize_model()\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        print(f'episode {i_episode}/{num_episodes}, iteration {t}/{MAX_ITERATIONS} loss: {loss}', end='\\r')\n",
    "    losses.append(loss)\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print()\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "\n",
    "x = np.arange(len(losses))\n",
    "plt.plot(x, losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_performance(runs, steps=10):\n",
    "    avg_improvement = 0\n",
    "    \n",
    "    for i in range(runs):\n",
    "        test_env = WTAEnv(n, m, lower_val, upper_val, prob, device)\n",
    "\n",
    "        init_value = test_env.value\n",
    "        improvement = 0\n",
    "        for j in range(steps):\n",
    "            state = test_env.get_state().unsqueeze(1).transpose(0, 1)\n",
    "            best_action = policy_net(state).max(1)[1]\n",
    "\n",
    "            _, change, _, _ = test_env.step(best_action)\n",
    "            improvement += change\n",
    "        \n",
    "        avg_improvement += (improvement / init_value)\n",
    "    \n",
    "    avg_improvement /= runs\n",
    "    \n",
    "    return avg_improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On average, how much does stepping according to the trained network improve the value of the assignment?\n",
    "# Is a percent improvement\n",
    "# any value >0 probably means some level of generalization\n",
    "print(test_performance(1000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS159_Project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
